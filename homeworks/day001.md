- [ ] 读《Numerical Linear Algebra》Lecture 1-2，理解范数是"长度"的推广

# L1

Numerical linear algebra is really `functional analysis`, but with the emphasis always on practical algorithmic ideas rather than mathematical technicalities.

Every linear map from $\mathbb{C}^n$ to $\mathbb{C}^m$ can be expressed as multiplication by an $m\times n$ matrix.

Let $A$ be an $m\times n$ matrix ($m$ rows, $n$ columns) and let $x$ be an $n$-dimentional column vector. Then the matrix-vector product $b=Ax$ is the $m$-dimensional column vector defined as follows:
$$b_i=\displaystyle{\sum_{j=1}^{n}}a_{ij}x_{j},\qquad i=1,\dots,m.\tag{1.1}$$

Let $a_j$ denote the $j$-th column of $A$, $(1.1)$ can be rewritten as:
$$b=Ax=\displaystyle{\sum^{n}_{j=1}x_ia_j}.\tag{1.2}$$

As mathematicians, we are used to viewing the formula $Ax=b$ as a statement that $A$ acts on $x$ to produce $b$. The formula $(1.2)$, by contrast, suggests the interpretation that $x$ acts on $A$ to produce $b$.

**Vandermonde Matrix**.
Fix a sequence of numbers $\{x_1,x_2,\dots,x_m\}$. If $p$ and $q$ are polynomials of degree $<n$ and $\alpha$ is a scaler, then $p+q$ and $\alpha p$ are also polynomials of degree $<n$. Moreover, the values of these polynomials at the points $x_i$ satisfy the following linearity properties:
$$(p+q)(x_i)=p(x_i)+q(x_i),\\(\alpha p)(x_i)=\alpha(p(x_i)).$$

> degree 是多项式的阶数（次数），即其最高次项的次数。

Thus the map from vectors of coefficients of polynomials $p$ of degree $<n$ to vector $(p(x_1),p(x_2),\dots,p(x_m))$ of sampled polynomial values is **linear**.

<u>Any linear map can be expressed as multiplication by a matrix.</u>

In fect, it is expressed by an $m\times n$ *Vandermonde matrix*:
$$A=\left[\begin{matrix}
1&x_1&x_1^2&\cdots&x_1^{n-1}\\
1&x_2&x_2^2&\cdots&x_2^{n-1}\\
\vdots&\vdots&\vdots&\ddots&\vdots\\
1&x_m&x_m^2&\cdots&x_m^{n-1}\\
\end{matrix}\right].$$

If $c$ is the column vector of coefficients of $p$,
$$c=\left[\begin{matrix}c_0\\c_1\\c_2\\\vdots\\c_{n-1}\end{matrix}\right],\qquad p(x)=c_0+c_1x+c_2x^2+\dots+c_{n-1}x^{n-1},$$
then the product $Ac$ gives the sampled polynomial values. That is for each $i$ from $1$ to $m$, we have
$$(Ac)_i=c_0+c_1x_i+c_2x_i^2+\dots+c_{n-1}x_i^{n-1}=p(x_i).\tag{1.3}$$

In this example, it is clear that the matrix-vector product $Ac$ need not be thought of as $m$ distinct scalar summations, each giving a different linear combination of the entries of $c$, as $(1.1)$ might suggest. Instead, $A$ can be viewed as a matrix of columns, each giving sampled values of a monomial,
$$A=\left[1|x|x^2|\cdots|x^{n-1}\right],\tag{1.4}$$
and the product $Ac$ should be understood as a single vector summation in the form of $(1.2)$ that at once gives a linear combination of these monomials,
$$Ac=c_0+c_1x+c_2x^2+\dots+c_{n-1}x^{n-1}=p(x).$$

For the matrix-matrix product $B=AC$, <u>each column of $B$ is a linear combination of the columns of $A$</u>. 
To derive this fact, we can begin it with the usual formula for matrix products.

If $A$ is $l\times m$ and $C$ is $m\times n$, then $B$ is $l\times n$, with entries defined by:
$$b_{ij}=\displaystyle{\sum_{k=1}^ma_{ik}c_{kj}}.\tag{1.5}$$

Just like $(1.2)$, the value of $b_j$ can be viewed as 
$$b_j=Ac_j=\displaystyle{\sum_{k=1}^m}c_{kj}a_k.\tag{1.6}$$

**Outer Product.**
A simple example of a matrix-matrix product is the <u>outer product.</u>. 
This is the product of an $m$-dimensional column vector $u$ with an $n$-dimensional row vector $v$; the result is an $m\times n$ matrix of rank 1.

The outer product can be written as:
$$\Biggl[u\Biggl]
\left[\begin{array}{ccc}
v_1 & v_2 & \cdots & v_n
\end{array}\right]
=
\Biggl[v_1 u \Biggl| v_2 u \Biggl| \cdots \Biggl| v_n u\Biggl]
=
\left[\begin{array}{ccc}
v_1 u_1 & \cdots & v_n u_1 \\
\vdots &  & \vdots \\
v_1 u_m & \cdots & v_n u_m
\end{array}\right]
$$
The columns are all multiples of the same vector $u$, and similarly, the rows are all multiples of the same vector $v$.

As a second illustration, consider $B=AR$, where $R$ is the upper-triangular $n\times n$ matrix with entries $r_{ij}=1$ for $i\le j$ and $r_{ij}=0$ for $i>j$.
This product can be written as:
$$\Biggl[b_1\Biggl|\cdots\Biggl|b_n\Biggl]=\Biggl[a_1\Biggl|\cdots\Biggl|a_n\Biggl]\left[\begin{matrix}
1&\cdots&1\\
&\ddots&1\\
&&1
\end{matrix}\right].$$

The column formula $(1.6)$ now gives:
$$b_j=Ar_j=\displaystyle{\sum_{k=1}^ja_k}.\tag{1.7}$$

That is the $j$-th column of $B$ is the sum of the first $j$ columns of $A$.
The matrix $R$ is a discrete analogue of an indefinite integral operator.

**Range and Nullspace**
The *range* of a matrix $A$, written as $\mathrm{range}(A)$, is the set of vectors that can be expressed as $Ax$ for some $x$.

The formula $(1.2)$ leads naturally to the following characterization of $\mathrm{range}(A)$.

**Theorem 1.1.** *$\mathrm{range}(A)$ is the space spanned by the columns of $A$*.

*Proof.* By $(1.2)$, any $Ax$ is a linear combination of the columns of $A$. Conversely, any vector $y$ in the space spanned by the columns of $A$ can be written as a linear combination of the columns, $y=\sum_{j=1}^nx_ja_j.$ Forming a vector $x$ out of the coefficients $x_j$, we have $y=Ax$, and thus $y$ is in the range of $A$.

> 这里的 span 可以翻译做“张成/张成空间”。它描述的是**向量集合能生成的全部空间**。可以口语化的描述为<u>Span 是列向量们的“势力范围”，而 $Ax$ 是拿系数向量 $x$ 在这个范围里“调”出来的具体结果。</u>

The range of a matrix $A$ is also called the *column space* of A.

The *nullspace* of $A\in\mathbb C^{m\times n}$, written $\mathrm{null}(A)$, is the set of vectors $x$ that satify $Ax=0$, where 0 is the 0-vector in $\mathbb C^m$.

The entries of each vector $x\in\mathrm{null}(A)$ give the coefficients of an expansion of zero as a linear combination of columns of $A: 0=x_1a_1+x_2a_2+\cdots+x_na_n.$

> 零空间又被称作核空间 (kernel)，记作 $\mathrm{ker}(A)$. 人话总结说<u>零空间是矩阵 $A$ 的“核”：是所有输入向量中，被 $A$ 乘后会消失不见的那些向量组成的子空间。</u>

**Rank**
The *column rank* of a matrix is the dimension of its column space.
Similarly, the *row rank* of a matrix is the dimension of the space spanned by its rows.
Row rank always equals column rank, so we refer to this number simply as the *rank* of a matrix.

An $m\times n$ matrix of *full rank* is one that has the maximal possible rank (`min(m, n)`).
This means that a matrix of full rank with $m\ge n$ must have $n$ linearly independent columns.
Such a matrix can also be characterized by the property that the map it defines is one-to-one.

**Theorem 1.2.** *A matrix $A\in\mathbb C^{m\times n}$ has full rank if and only if it maps no two distinct vectors to the same vector.*

*Proof.* ($\Rightarrow$) If $A$ is of full rank, its columns are linearly independent, so they form a basis for $\mathrm{range}(A)$. This means that every $b\in\mathrm{range}(A)$ has a unique linear expansion in terms of the columns of $A$, and therefore, by $(1.2)$, every $b\in\mathrm{range}(A)$ has a unique $x$ such that $b=Ax$. ($\Leftarrow$)
Conversely, if $A$ is not of full rank, its columns $a_j$ are dependent, and there is a nontrivial linear combination such that $\sum_{j=1}^nc_ja_j=0$. The nonzero vector $c$ formed from the coefficients $c_j$ satisfies $Ac=0$. But then $A$ maps distinct vectors to the same vector since, for any $x$, $Ax=A(x+c)$.

**Inverse**
A *nonsingular* or *invertible* matrix is a square matrix of full rank.
Note that the $m$ columns of a nonsingular $m\times m$ matrix $A$ form a basis for the whole space $\mathbb C^m$.
Therefore, we can uniquely express any vector as as linear combination of them.
In particular, the canonical unit vector with $1$ in the $j$-th entry and zeros elsewhere, written $e_j$, can be expanded:
$$e_j=\displaystyle{\sum_{i=1}^mz_{ij}a_i}.\tag{1.8}$$
Let $Z$ be the matrix with entries $z_{ij}$, and let $z_j$ denote the $j$-th column of $Z$.
Then $(1.8)$ can be written as $e_j=Az_j$. This equation has the form $(1.6)$; it can be written again, most concisely, as
$$\Biggl[e_1\Biggl|\cdots\Biggl|e_m\Biggl]=I=AZ,$$
where $I$ is the $m\times m$ matrix known as the *identity*.
The matrix $Z$ is the *inverse* of $A$.
Any square nonsigular matrix $A$ has a unique inverse, written $A^{-1}$, that satisfies $AA^{-1}=A^{-1}A=I$.

**Theorem 1.3.**
*For $A\in\mathbb C^{m\times m}$, the following conditions are equivalent:*
1. *A has an inverse $A^{-1}$*,
2. *$\mathrm{rank}(A)=m$*,
3. *$\mathrm{range}(A)=\mathbb C^m$*,
4. *$\mathrm{null}(A)={0}$*,
5. *$0$ is not an eigenvalue of $A$*,
6. *$0$ is not a singular value of $A$*,
7. *$\mathrm{det}(A)\neq 0$*.

When writing the product $x=A^{-1}b$, it is important not to let the inverse-matrix notation obscure what is really going on!
Rather than thinking of $x$ as the result of applying $A^{-1}$ to $b$, we should understand it as the unique vector that satisfies the equation $Ax=b$. By $(1.2)$, this means that $x$ is the vector of coefficients of the unique linear expansion of $b$ in the basis of columns of A.

This point cannot be emphasized too much, so we repeat:

**$A^{-1}b$ is the vector of coefficients of the expansion of $b$ in the basis of columns of $A$.**

Multiplication by $A^{-1}$ is a *change of basis* operation:

If we watch $b$ as coefficients of the expansion of $b$ in $\{e_1,\cdots,e_m\}$, then the multiplication of $b$ by $A^{-1}$ is $A^{-1}b$ which means the coefficients of the expansion of $b$ in $\{a_1,\cdots,a_m\}$.

Conversely, the multiplication of $A^{-1}b$ by $A$ is $AA^{-1}b=b$.

## L1 总结
首先应该培养的思维是，不要再习惯性的按传统方法去看待矩阵乘向量 $(Ax)$ 或是矩阵乘矩阵 $(AB)$ 了，而应该尝试以$(1.2)$的方式去看待。

传统的思维即是 $A$ 行去乘 $x_i$ 或是 $B$ 列，去直接得到该位置上的元素。

而新的思想是通过 $x_i$ 或是 $B$ 行去乘 $A$ 列，得到 $A$ 在该位置的一部分。

比如 $A\in R^{4\times3}$，$B\in R^{3\times}2$，传统思想求 $AB$ 就是直接取 $A$ 行这样的 $1\times3$ 去乘 $B$ 列的 $3\times1$ 得到一个具体的元素。而新的思维应该是反过来用 $A$ 列这样的 $4\times1$ 去乘 $B$ 行的 $1\times 2$，通过外积的形式得到一个 $4\times 2$. 最后再把求得的 $3$ 个 $4\times 2$ 加起来得到最终的 $4\times2$.

矩阵的值域是所有通过矩阵 $A$ 的线性变换能得到的向量集合。

矩阵的零空间（核空间）是通过该矩阵会映射到零的 $x$ 集合。

秩是一个矩阵的值域的维度，列秩和行秩对应的值是一样的。秩代表一个矩阵的有效信息比率，满秩即是该矩阵的秩等于行数和列数中小的那个值，代表该矩阵没有冗余信息。秩亏则相反，秩亏意味着矩阵的列（或行）之间存在线性相关性，即某些列（或行）可以被其他列（或行）线性表示，因此矩阵的“自由度”减少了。

> “自由度” degrees of freedom 可以理解为“你可以自由赋值而不影响约束条件的变量个数。”它量化的就是矩阵“还能往哪个方向扰动而不掉秩。”

从几何学的角度理解，矩阵的秩代表矩阵所能表示的“独立方向”的数量。

满秩和秩亏在方阵中对应非奇异和奇异。

一个矩阵是秩亏的，就意味着该矩阵中存在荣誉信息，意味着某些行或者列可以转换为全零，意味着其零空间非凡（即存在非零向量可以被映射到零向量）。当其是方阵是意味着其行列式为零，其属于奇异矩阵，奇异矩阵不可逆。

简单对于方阵来说：
- 满秩（非奇异）一定可逆，可逆一定满秩（非奇异）；
- 秩亏（奇异）一定不可逆，不可逆一定秩亏（奇异）；

一个非奇异的方阵，其列向量就是其整个值域空间的**完美坐标系**。
> 因为在 $m$ 维空间 $\mathbb C^m$ 中，任何 $m$ 个线性无关的向量自动构成基。

一个方阵是非奇异的，就意味着满秩，意味着其中向量线性无关，意味着行列式不为零，意味着零空间只有零向量，意味着可以形成一个基，意味着其可以张满整个空间，意味着其可以映射到空间中的任意向量，意味着也可以映射到单位向量。
将能够使其映射到单位向量的那些向量按列向量的形式组合起来形成的新矩阵就是该矩阵的逆。


## 疑问
这样做的目的可能是后期通过计算机算起来快？

## 回答
Kimi 介绍这样做的优势：
1. **揭示矩阵的“列空间”本质**
    - $AB$ 的每一列都是 $A$ 的列向量的线性组合：
        $(AB)_{:,j}=A\cdot B_{:,j}=\displaystyle{\sum_{k=1}^{3}B_{kj}\cdot A_{:,k}}$
    - 这直接提现了：**矩阵乘向量=列向量的线性组合**
    - 这对理解**秩、零空间、像空间**至关重要
2. **秩 1 分解的普适性**
    - 任何矩阵乘积都可以看作**秩 1 矩阵的和**：
        $AB=\displaystyle{\sum_{k=1}^r\mathrm{rank-1矩阵}}$
    - 这与 SVD、LU 等分解的思想一脉相承，数值算法中**低秩更新**是核心操作
3. **数值稳定性与舍入误差**
    - 在迭代算法中，逐步积累外积可以**延迟舍入误差的积累**。例如：
        - 计算 $A^TA$ 时，传统方法可能更快失去正交性
        - 外积求和可以配合**分块算法**，利用更高精度的累加器
4. **计算效率与内存访问**
    - 现代计算机是**列优先存储**（Fortran/MATLAB风格）
        - 连续访问 $A$ 的列 `A[:,k]` 是内存友好的（缓存命中率高）
        - 而跳行访问 `A[i, :]` 可能造成**缓存失效**，降低性能
5. 并行化友好
    - 每个外积 $A_{:,k}\cdot B_{k,:}$ 相互独立，可以并行计算后累加，而传统点积的依赖关系更强。

总结来说就是：**这种视角将矩阵乘法从“逐点计算”提升为“结构操作”**。
数值线性代数的所有高级算法（QR、SVD、特征值计算）都建立在这种**列空间+秩 1 更新**的洞察秩上。培养这种思维，是理解**矩阵计算稳定性、复杂度和结构设计**的第一步。


# L2
